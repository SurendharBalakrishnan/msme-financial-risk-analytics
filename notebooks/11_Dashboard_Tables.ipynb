{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dashboard-title",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Page 3 Dashboard - ML Model Tables\n",
    "This notebook creates the tables needed for the ML Model dashboard visualizations:\n",
    "- Model Comparison (LR/RF/GBT)\n",
    "- Feature Importance\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "model-comparison-header",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "model-comparison-code",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model Comparison Data from 08_ML_Training results\n",
    "model_comparison_data = [\n",
    "    (\"Logistic Regression\", 0.5936, 0.7349, \"LR\", 1),\n",
    "    (\"Random Forest\", 0.6432, 0.7603, \"RF\", 2),\n",
    "    (\"Gradient Boosted Trees\", 0.6955, 0.7613, \"GBT\", 3)\n",
    "]\n",
    "\n",
    "model_comparison = spark.createDataFrame(\n",
    "    model_comparison_data, \n",
    "    [\"model_name\", \"roc_auc\", \"accuracy\", \"model_code\", \"model_order\"]\n",
    ")\n",
    "\n",
    "model_comparison.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"msme_risk_analytics.gold_model_comparison\")\n",
    "\n",
    "print(\"âœ… Model comparison table created!\")\n",
    "spark.table(\"msme_risk_analytics.gold_model_comparison\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feature-importance-header",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Feature Importance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feature-importance-code",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature Importance from GBT Model (09_Model_Evaluation results)\n",
    "feature_importance_data = [\n",
    "    (\"LTV\", 0.255088, 1),\n",
    "    (\"loan_to_income_ratio\", 0.192788, 2),\n",
    "    (\"dtir1\", 0.173469, 3),\n",
    "    (\"loan_amount\", 0.145198, 4),\n",
    "    (\"income\", 0.102258, 5),\n",
    "    (\"Credit_Score\", 0.075848, 6),\n",
    "    (\"risk_score\", 0.055350, 7)\n",
    "]\n",
    "\n",
    "feature_importance = spark.createDataFrame(\n",
    "    feature_importance_data,\n",
    "    [\"feature_name\", \"importance\", \"rank\"]\n",
    ")\n",
    "\n",
    "feature_importance.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"msme_risk_analytics.gold_feature_importance\")\n",
    "\n",
    "print(\"âœ… Feature importance table created!\")\n",
    "spark.table(\"msme_risk_analytics.gold_feature_importance\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "confusion-matrix-header",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Confusion Matrix Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "confusion-matrix-code",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix from GBT Model (09_Model_Evaluation results)\n",
    "confusion_matrix_data = [\n",
    "    (0, 0, 2133, \"True Negative\", \"Correctly predicted No Default\"),\n",
    "    (0, 1, 39, \"False Positive\", \"Incorrectly predicted Default\"),\n",
    "    (1, 0, 684, \"False Negative\", \"Missed Default (High Risk!)\"),\n",
    "    (1, 1, 173, \"True Positive\", \"Correctly predicted Default\")\n",
    "]\n",
    "\n",
    "confusion_matrix = spark.createDataFrame(\n",
    "    confusion_matrix_data,\n",
    "    [\"actual_status\", \"predicted_status\", \"count\", \"category\", \"description\"]\n",
    ")\n",
    "\n",
    "confusion_matrix.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"msme_risk_analytics.gold_confusion_matrix\")\n",
    "\n",
    "print(\"âœ… Confusion matrix table created!\")\n",
    "spark.table(\"msme_risk_analytics.gold_confusion_matrix\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "metrics-summary-header",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Model Metrics Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "metrics-summary-code",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate detailed metrics for GBT (Best Model)\n",
    "# From confusion matrix: TP=173, TN=2133, FP=39, FN=684\n",
    "TP = 173\n",
    "TN = 2133\n",
    "FP = 39\n",
    "FN = 684\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "roc_auc = 0.6955\n",
    "\n",
    "metrics_data = [\n",
    "    (\"GBT\", accuracy, precision, recall, f1_score, roc_auc, 11758, 3029)\n",
    "]\n",
    "\n",
    "metrics_summary = spark.createDataFrame(\n",
    "    metrics_data,\n",
    "    [\"model\", \"accuracy\", \"precision\", \"recall\", \"f1_score\", \"roc_auc\", \"train_records\", \"test_records\"]\n",
    ")\n",
    "\n",
    "metrics_summary.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"msme_risk_analytics.gold_model_metrics_summary\")\n",
    "\n",
    "print(\"âœ… Model metrics summary table created!\")\n",
    "spark.table(\"msme_risk_analytics.gold_model_metrics_summary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "verify-header",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Verify All Tables Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "verify-code",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š PAGE 3 DASHBOARD TABLES SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tables = [\n",
    "    \"gold_model_comparison\",\n",
    "    \"gold_feature_importance\", \n",
    "    \"gold_confusion_matrix\",\n",
    "    \"gold_model_metrics_summary\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    count = spark.table(f\"msme_risk_analytics.{table}\").count()\n",
    "    print(f\"âœ… msme_risk_analytics.{table}: {count} rows\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽ‰ All tables ready for Page 3 Dashboard!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "sql-queries-header",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SQL Queries for Dashboard Visualizations\n",
    "\n",
    "Use these SQL queries in your Databricks Dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "sql-model-comparison",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query 1: Model Comparison (Bar Chart)"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Model Performance Comparison\n",
    "# SELECT \n",
    "#     model_name,\n",
    "#     model_code,\n",
    "#     ROUND(roc_auc * 100, 2) as roc_auc_pct,\n",
    "#     ROUND(accuracy * 100, 2) as accuracy_pct\n",
    "# FROM msme_risk_analytics.gold_model_comparison\n",
    "# ORDER BY roc_auc DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "sql-feature-importance",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query 2: Feature Importance (Horizontal Bar Chart)"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Feature Importance\n",
    "# SELECT \n",
    "#     feature_name,\n",
    "#     ROUND(importance * 100, 2) as importance_pct,\n",
    "#     rank\n",
    "# FROM msme_risk_analytics.gold_feature_importance\n",
    "# ORDER BY importance DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "sql-confusion-matrix",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query 3: Confusion Matrix (Table/Heatmap)"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Confusion Matrix\n",
    "# SELECT \n",
    "#     CASE WHEN actual_status = 0 THEN 'No Default' ELSE 'Default' END as actual,\n",
    "#     CASE WHEN predicted_status = 0 THEN 'No Default' ELSE 'Default' END as predicted,\n",
    "#     count,\n",
    "#     category\n",
    "# FROM msme_risk_analytics.gold_confusion_matrix\n",
    "# ORDER BY actual_status, predicted_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "sql-kpi-metrics",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query 4: KPI Metrics (Counter Widgets)"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Model KPI Metrics\n",
    "# SELECT \n",
    "#     ROUND(accuracy * 100, 1) as accuracy_pct,\n",
    "#     ROUND(precision * 100, 1) as precision_pct,\n",
    "#     ROUND(recall * 100, 1) as recall_pct,\n",
    "#     ROUND(roc_auc * 100, 1) as roc_auc_pct\n",
    "# FROM msme_risk_analytics.gold_model_metrics_summary\n",
    "# WHERE model = 'GBT'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10_Dashboard_Tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
